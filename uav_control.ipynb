{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAV_env(gym.Env):\n",
    "    def __init__(self):\n",
    "        # UAV parameters\n",
    "        self.m = 1.56  # Mass (kg)\n",
    "        self.Jyy = 0.0576  # Moment of inertia (kg-m^2)\n",
    "        self.CL = lambda a: (3.5*a) + 0.09  # Lift coefficient\n",
    "        self.CD = lambda a: (0.2*a) + 0.016  # Drag coefficient\n",
    "        self.CLdelta = 0.27  # Lift coefficient derivative\n",
    "        self.Cm0 = -0.02  # Pitching moment coefficient at zero angle of attack\n",
    "        self.Cmalpha = -0.57  # Pitching moment coefficient derivative\n",
    "        self.Cmq = -1.4  # Pitching moment coefficient derivative\n",
    "        self.Cmdelta = -0.32  # Pitching moment coefficient derivative\n",
    "\n",
    "        # Initial state\n",
    "        self.X0 = np.array([9.96, 0.87, 0, 0.0873, 50])  # [u, w, q, theta, h]\n",
    "        self.U0 = np.array([1.0545, -0.2179])  # [T, delta]\n",
    "\n",
    "        # Desired final state\n",
    "        self.Xd = np.array([9.85, 1.74, 0, 0.1745, 62])\n",
    "\n",
    "        # State and control constraints\n",
    "        self.T_min = 0\n",
    "        self.T_max = 4\n",
    "        self.alpha_min = 0\n",
    "        self.alpha_max = 0.2618\n",
    "        self.Va_min = 5\n",
    "        self.Va_max = 15\n",
    "        self.delta_min = -0.4363\n",
    "        self.delta_max = 0.1745\n",
    "\n",
    "        # Simulation time\n",
    "        self.dt = 0.02  # Sampling time (seconds)\n",
    "        self.t_final = 10  # Final time (seconds)\n",
    "        self.t = 0\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Box(low=np.array([self.T_min, self.delta_min]),\n",
    "                                       high=np.array([self.T_max, self.delta_max]),\n",
    "                                       dtype=np.float32)\n",
    "        \n",
    "        # self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32)\n",
    "\n",
    "        self.state = None\n",
    "\n",
    "    def dynamics(self, X, U):\n",
    "        u, w, q, theta, h = X       # States\n",
    "        T, delta = U                # Controls\n",
    "\n",
    "        Va = np.sqrt(u**2 + w**2)   # Airspeed (need to check this to keep within constraints)\n",
    "        alpha = np.arctan(w / u)    # Angle of attack (need to check this to keep within constraints)\n",
    "\n",
    "        CL = self.CL(alpha)         # Lift coefficient\n",
    "        CD = self.CD(alpha)         # Drag coefficient\n",
    "\n",
    "        udot = (-q*w) + (0.5*Va**2/self.m) * (CL*np.sin(alpha) - CD*np.cos(alpha) + self.CLdelta*np.sin(alpha)*delta) - (9.81*np.sin(theta)) + (T/self.m)\n",
    "        wdot = (q*u) + (0.5*Va**2/self.m) * (-CL*np.cos(alpha) - CD*np.sin(alpha) - self.CLdelta*np.cos(alpha)*delta) + (9.81*np.cos(theta))\n",
    "        qdot = (0.5*Va**2/self.Jyy) * (self.Cm0 + self.Cmalpha*alpha + (0.25*self.Cmq*q/Va) + self.Cmdelta*delta)\n",
    "        thetadot = q\n",
    "        hdot = (-u*np.sin(theta)) + (w*np.cos(theta))\n",
    "\n",
    "        return np.array([udot, wdot, qdot, thetadot, hdot]) # Return state derivatives\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "        self.state = self.integrate(self.state, action) \n",
    "        u, w, _, _, _ = self.state\n",
    "        Va = np.sqrt(u**2 + w**2)\n",
    "        alpha = np.arctan(w / u)\n",
    "        if Va < self.Va_min or Va > self.Va_max or alpha < self.alpha_min or alpha > self.alpha_max:\n",
    "            reward = -np.inf  # Penalize out of bounds states\n",
    "            done = True\n",
    "        reward = -np.sum(np.abs(self.state - self.Xd)) # Negative sum of absolute state errors\n",
    "        done = False\n",
    "        if self.t >= 10:\n",
    "            done = True\n",
    "        info = {} # store additional metadata for debugging \n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.copy(self.X0)\n",
    "        self.t = 0\n",
    "        return self.state\n",
    "\n",
    "    def integrate(self, X, U):\n",
    "        X_dot = self.dynamics(X, U)\n",
    "        X += self.dt * X_dot\n",
    "        self.t += self.dt\n",
    "        return X\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # print(self.state)  \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode score: -10376.619591643122\n",
      "Episode score: -25.56321200652404\n",
      "Episode score: -29.334448757309257\n",
      "Episode score: -25.743912355925083\n",
      "Episode score: -25.140450336038814\n",
      "Episode score: -24.642498488287693\n",
      "Episode score: -30.054335608523424\n",
      "Episode score: -28.980152668376796\n",
      "Episode score: -24.430714220787614\n",
      "Episode score: -26.923249432302356\n"
     ]
    }
   ],
   "source": [
    "env = UAV_env()\n",
    "state = env.reset()\n",
    "num_episodes = 10\n",
    "for _ in range(num_episodes):\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        if env.t == 0:\n",
    "            action = env.U0\n",
    "        else:\n",
    "            # action = DQN(state)  # Use DQN in keras-rl2 to get our actual action policy\n",
    "            action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print(f\"Episode score: {score}\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
