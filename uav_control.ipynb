{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAV_env(gym.Env):\n",
    "    def __init__(self):\n",
    "        # UAV parameters\n",
    "        self.m = 1.56  # Mass (kg)\n",
    "        self.Jyy = 0.0576  # Moment of inertia (kg-m^2)\n",
    "        self.CL = lambda a: (3.5*a) + 0.09  # Lift coefficient\n",
    "        self.CD = lambda a: (0.2*a) + 0.016  # Drag coefficient\n",
    "        self.CLdelta = 0.27  # Lift coefficient derivative\n",
    "        self.Cm0 = -0.02  # Pitching moment coefficient at zero angle of attack\n",
    "        self.Cmalpha = -0.57  # Pitching moment coefficient derivative\n",
    "        self.Cmq = -1.4  # Pitching moment coefficient derivative\n",
    "        self.Cmdelta = -0.32  # Pitching moment coefficient derivative\n",
    "\n",
    "        # Initial state\n",
    "        self.X0 = np.array([9.96, 0.87, 0, 0.0873, 50])  # [u, w, q, theta, h]\n",
    "        self.U0 = np.array([1.0545, -0.2179])  # [T, delta]\n",
    "\n",
    "        # Desired final state\n",
    "        self.Xd = np.array([9.85, 1.74, 0, 0.1745, 62])\n",
    "\n",
    "        # State and control constraints\n",
    "        self.T_min = 0\n",
    "        self.T_max = 4\n",
    "        self.alpha_min = 0\n",
    "        self.alpha_max = 0.2618\n",
    "        self.Va_min = 5\n",
    "        self.Va_max = 15\n",
    "        self.delta_min = -0.4363\n",
    "        self.delta_max = 0.1745\n",
    "\n",
    "        # Simulation time\n",
    "        self.dt = 0.02  # Sampling time (seconds)\n",
    "        self.t_final = 10  # Final time (seconds)\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Box(low=np.array([self.T_min, self.delta_min]),\n",
    "                                       high=np.array([self.T_max, self.delta_max]),\n",
    "                                       dtype=np.float32)\n",
    "        # self.action_space = spaces.Discrete(3,3) ??? discrete action space \n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32)\n",
    "\n",
    "        self.state = None\n",
    "\n",
    "    def dynamics(self, X, U):\n",
    "        u, w, q, theta, h = X       # States\n",
    "        T, delta = U                # Controls\n",
    "\n",
    "        Va = np.sqrt(u**2 + w**2)   # Airspeed (need to check this to keep within constraints)\n",
    "        alpha = np.arctan(w / u)    # Angle of attack (need to check this to keep within constraints)\n",
    "\n",
    "        CL = self.CL(alpha)         # Lift coefficient\n",
    "        CD = self.CD(alpha)         # Drag coefficient\n",
    "\n",
    "        udot = (-q*w) + (0.5*Va**2/self.m) * (CL*np.sin(alpha) - CD*np.cos(alpha) + self.CLdelta*np.sin(alpha)*delta) - (9.81*np.sin(theta)) + (T/self.m)\n",
    "        wdot = (q*u) + (0.5*Va**2/self.m) * (-CL*np.cos(alpha) - CD*np.sin(alpha) - self.CLdelta*np.cos(alpha)*delta) + (9.81*np.cos(theta))\n",
    "        qdot = (0.5*Va**2/self.Jyy) * (self.Cm0 + self.Cmalpha*alpha + (0.25*self.Cmq*q/Va) + self.Cmdelta*delta)\n",
    "        thetadot = q\n",
    "        hdot = (-u*np.sin(theta)) + (w*np.cos(theta))\n",
    "\n",
    "        return np.array([udot, wdot, qdot, thetadot, hdot]) # Return state derivatives\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "        self.state = self.integrate(self.state, action) \n",
    "        reward = -np.sum(np.abs(self.state - self.Xd))  # Negative sum of absolute state errors\n",
    "        done = False\n",
    "        if self.t_final <= 10:\n",
    "            done = True\n",
    "        info = {} # store additional information for debugging \n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.copy(self.X0)\n",
    "        return self.state\n",
    "\n",
    "    def integrate(self, X0, U):\n",
    "        X = np.copy(X0)\n",
    "        for _ in np.arange(0, self.t_final, self.dt):\n",
    "            X += self.dt * self.dynamics(X, U)\n",
    "        return X\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # print(self.state)  \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN(state):\n",
    "    # Deep Q-Network\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\ml\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "env = UAV_env()\n",
    "state = env.reset()\n",
    "\n",
    "num_episodes = 100\n",
    "for _ in range(num_episodes):\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = DQN(state)\n",
    "        state, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "Q = np.zeros((env.observation_space.shape[0], env.action_space.n))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Choose action using epsilon-greedy policy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Update Q-table\n",
    "        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "\n",
    "        state = next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
